# Job Market Dashboard - Take-Home Assessment for Coupang

## Test Overview and Objective for Crawling/Web Scraping Engineer
Extract structured data from a given website, handle potential challenges such
as pagination, or dynamic content, and present findings clearly. This test
evaluates your ability to design and implement a web scraping solution, use
appropriate tools, and communicate your approach and results effectively.
Estimated Time: 2-4 hours

## Task Description

1. Data Extraction
    1. Select a publicly accessible website that contains tabular or list-
based data (e.g., product listings, event schedules, or contact directories). Your goal is to scrape the following:
        - Key data fields
        - Handle pagination or load more buttons if applicable
        - Extract at least 100 records or all available if fewer

2. Data Output
   1. Save the extracted data in a structured format such as JSON or
CSV

3. Challenges Handling
    1. If the site uses JavaScript to load content dynamically, implement a solution to handle this (e.g., using Selenium or Scrapy with Splash). If the site has basic anti-scraping measures (like rate limiting), describe your approach to handle it.

4. Analysis & Summery
    1. Provide a brief analysis of the data you extracted. This could include insights like data distributions, notable patterns, or data quality issues.

## Deliverables
- [X] Code/Scripts used for scraping and data extraction, with clear instructions on how to run them
- [X] Extracted data file in JSON or CSV format
- [X] Written report (1-2 pages) including your overall approach and design - [report](/report.md)

---
Your README setup is already clean and direct, but it can be made even more concise, professional, and welcoming to contributors. Hereâ€™s an improved, slightly more polished version that maintains clarity and structure while adding brief context at each step, minimal redundancy, and a bit of friendlinessâ€”*without sacrificing any detail*.

---

## ğŸ“¦ Set up

> **Platform:** macOS (Bash).
> **Prerequisite:** Python 3.x installed.

### Install dependencies

```sh
pip install -r requirements.txt
```

### Run the scraper

```sh
python -m scraper.selenium_scraper
```

### Start the server

```sh
python app.py
```

### ğŸ§ª Run Tests

Install dev dependencies and run tests:

```sh
pip install -r requirements-dev.txt
pytest
```

### â° Schedule Scraper with Cron

Automate scraper execution via cron jobs:

```sh
chmod +x scripts/*
./scripts/install_cron.sh    # Add scheduled run
./scripts/uninstall_cron.sh  # Remove scheduled run
```

Check your cron jobs:

```sh
crontab -l
```

### ğŸ“‚ Outputs

Scraped data and generated charts will be saved to:

```
data/techinasia_jobs_*.csv
data/charts/*.png
```

> If you encounter issues, ensure all dependencies are installed and your Python version is correct.



---



## ğŸ“ Key Files & Structure

```text
.
â”œâ”€â”€ README.md                 # Setup & usage instructions
â”œâ”€â”€ report.md                 # Project writeup/report
â”œâ”€â”€ requirements.txt          # Core dependencies
â”œâ”€â”€ requirements-dev.txt      # Dev/test dependencies
â”œâ”€â”€ app.py                    # API/web server entrypoint
â”œâ”€â”€ logs/                     # Runtime logs
â”œâ”€â”€ _additional/              # Project docs, Dockerfiles, legacy scripts
â”œâ”€â”€ _learnings/               # Personal notes, not needed for review
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ techinasia_jobs_*.csv # Output: scraped jobs data
â”‚   â”œâ”€â”€ charts/               # Output: analysis plots
â”‚   â”‚   â””â”€â”€ *.png
â”‚   â””â”€â”€ quality_report.csv    # Data quality info
|
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ selenium_scraper.py   # Main Selenium scraping logic (**entry point**)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py           # Scrapy/Selenium settings (if used)
â”‚   â”œâ”€â”€ items.py, pipelines.py, middlewares.py  # (Scrapy modules, if used)
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ sg_jobs_spider.py # Spider logic (initial idea, for reference)
|
â”œâ”€â”€ data_processing.py        # Cleans & analyses scraped data
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ functions.py          # Shared helper functions
â”‚   â”œâ”€â”€ reporting.py          # Reporting utilities
â”‚   â”œâ”€â”€ constants.py          # Constants
|   â”œâ”€â”€ enums.py              # Enums
â”‚   â””â”€â”€ test_reporting.py     # Test for reporting utils
|
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install_cron.sh       # Add cron job
â”‚   â””â”€â”€ uninstall_cron.sh     # Remove cron job
|
â”œâ”€â”€ test_data_processing.py   # Tests for data processing
|
â”œâ”€â”€ templates/
    â””â”€â”€ index.html            # (If web frontend is used)

```

---

***Explanation for reviewers:***

* **scraper/selenium\_scraper.py** â€“ *Main scraping entry point*.
* **data\_processing.py** â€“ *Cleans and analyses data*.
* **data/** â€“ *All output data and generated plots are saved here*.
* **scripts/** â€“ *Cron job management*.
* **utils/** â€“ *Reusable functions and reporting helpers*.
* **README.md / report.md** â€“ *Instructions and technical writeup*.

---

**Unsure/Extra (review if needed):**

* `app.py` (if not running as a web server, can ignore)
* `spiders/`, `items.py`, `pipelines.py`, `middlewares.py` (if not using Scrapy)
* `_additional/`, `_learnings/` (project notes/docs, not needed for build/run)
* `logs/` (helpful for debug, not core logic)



---



## References
- https://www.techinasia.com/jobs/search?country_name[]=Singapore&country_name[]=Remote&job_type[]=Full-time&job_type[]=Freelance&currency=SGD
- https://www.techinasia.com/robots.txt
- https://www.techinasia.com/sitemap.xml
